# -*- coding: utf-8 -*-
"""Agentic_RAG_Agile-Task Management.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TIx7pkvuIUNUBPW2KqoMwKC2CE9WKROt

# Agentic Retrieval-Augmented Generation System for Taiga Agile Project Data

# Stage 1 — Data Pre-processing

##Preprocessing
"""

!pip install pandas numpy

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/RAG"

import os

print(os.path.exists('/content/drive/My Drive/RAG/AMEP2324_EduConnect_user_stories.csv'))
print(os.path.exists('/content/drive/My Drive/RAG/AMEP2324_EduConnect_tasks.csv'))

us_path = '/content/drive/My Drive/RAG/AMEP2324_EduConnect_user_stories.csv'
task_path = '/content/drive/My Drive/RAG/AMEP2324_EduConnect_tasks.csv'

with open(us_path, 'r', encoding='utf-8') as f:
    for _ in range(15):
        print(f.readline())

import pandas as pd

us_df = pd.read_csv(
    us_path,
    sep=';',                 # Correct delimiter
    encoding='utf-8',        # Use 'utf-8-sig' if there are encoding issues
    quotechar='"',           # To handle multi-line text enclosed in quotes
    engine='python',         # Use Python engine to avoid C parser issues
    on_bad_lines='skip'      # Skip malformed lines if any exist
)

task_df = pd.read_csv(
    task_path,
    sep=';',                 # Correct delimiter
    encoding='utf-8',        # Use 'utf-8-sig' if there are encoding issues
    quotechar='"',           # To handle multi-line text enclosed in quotes
    engine='python',         # Use Python engine to avoid C parser issues
    on_bad_lines='skip'      # Skip malformed lines if any exist
)

us_df.head()

task_df.head()

# Check the shape (rows, columns) of each DataFrame
print("User Stories shape:", us_df.shape)
print("Tasks shape:", task_df.shape)

# Check the percentage of missing values in each column
print("Missing values in User Stories:")
print(us_df.isnull().mean().sort_values(ascending=False))

print("Missing values in Tasks:")
print(task_df.isnull().mean().sort_values(ascending=False))

!pip install deep-translator --quiet

from deep_translator import GoogleTranslator
import pandas as pd
from IPython.display import display

# Safe translation function
def safe_translate(text):
    try:
        return GoogleTranslator(source='auto', target='en').translate(text)
    except:
        return text

### Translate all string columns in User Stories (us_df)
us_sample = us_df.head(3).copy()  # Take a sample of 3 rows
us_translated = us_sample.copy()

for col in us_sample.columns:
    if us_sample[col].dtype == "object":
        us_translated[col + "_en"] = us_sample[col].apply(safe_translate)

### Translate all string columns in Tasks (task_df)
task_sample = task_df.head(3).copy()
task_translated = task_sample.copy()

for col in task_sample.columns:
    if task_sample[col].dtype == "object":
        task_translated[col + "_en"] = task_sample[col].apply(safe_translate)

### Display Results
print("Translated Sample - User Stories:")
display(us_translated)

print("\n Translated Sample - Tasks:")
display(task_translated)

from deep_translator import GoogleTranslator
import pandas as pd
from IPython.display import display

# Safe translation function
def safe_translate(text):
    try:
        return GoogleTranslator(source='auto', target='en').translate(text)
    except:
        return text  # Return original text if translation fails

### Translate all text columns in the full User Stories DataFrame
us_translated = us_df.copy()

for col in us_df.columns:
    if us_df[col].dtype == "object":  # Only translate string columns
        us_translated[col + "_en"] = us_df[col].apply(safe_translate)

### Translate all text columns in the full Tasks DataFrame
task_translated = task_df.copy()

for col in task_df.columns:
    if task_df[col].dtype == "object":  # Only translate string columns
        task_translated[col + "_en"] = task_df[col].apply(safe_translate)

### Display translated data samples
print("Translated User Stories (sample):")
display(us_translated.head())

print("\n Translated Tasks (sample):")
display(task_translated.head())

print("Columns in translated_us_df:")
print(us_translated.columns.tolist())

print("\n Columns in translated_task_df:")
print(task_translated.columns.tolist())

# For User Stories
base_cols_us = us_translated.columns.tolist()
cols_to_remove_us = [col for col in base_cols_us if col.endswith('_en') == False and col + '_en' in base_cols_us]
clean_us_df = us_translated.drop(columns=cols_to_remove_us)

# For Tasks
base_cols_task = task_translated.columns.tolist()
cols_to_remove_task = [col for col in base_cols_task if col.endswith('_en') == False and col + '_en' in base_cols_task]
clean_task_df = task_translated.drop(columns=cols_to_remove_task)

# Optional: reset column order (move translated cols to front)
clean_us_df = clean_us_df[[col for col in clean_us_df.columns if col.endswith('_en')] + [col for col in clean_us_df.columns if not col.endswith('_en')]]
clean_task_df = clean_task_df[[col for col in clean_task_df.columns if col.endswith('_en')] + [col for col in clean_task_df.columns if not col.endswith('_en')]]

clean_us_df.head(5)

clean_task_df.head(5)

print(" Final columns in cleaned User Stories DataFrame:")
print(clean_us_df.columns.tolist())

print("\n Final columns in cleaned Tasks DataFrame:")
print(clean_task_df.columns.tolist())

# Check if the cleaned DataFrames contain any rows
print("clean_us_df has data:", not clean_us_df.empty)
print("clean_task_df has data:", not clean_task_df.empty)

# Optionally, print the shape (rows, columns) for additional context
print("clean_us_df shape:", clean_us_df.shape)
print("clean_task_df shape:", clean_task_df.shape)

# Merge tasks with their corresponding user stories using 'us_id'
merged_df = pd.merge(
    clean_task_df,
    clean_us_df,
    on='us_id',
    how='left',  # Keep all tasks even if a matching user story is not found
    suffixes=('_task', '_us')  # Add suffixes to distinguish overlapping columns
)

print("Merged DataFrame (Tasks + User Stories):")
merged_df.head()

# Drop task-side versions of the shared columns
merged_df.drop(columns=['project_id_task', 'milestone_id_task'], inplace=True)

# Optionally rename 'project_id_us' and 'milestone_id_us' back to simple names
merged_df.rename(columns={
    'project_id_us': 'project_id',
    'milestone_id_us': 'milestone_id'
}, inplace=True)

# Display updated columns
print(" Final columns in merged_df:")
print(merged_df.columns.tolist())

print(" Columns in merged_df:")
print(merged_df.columns.tolist())

merged_df.head(5)

# Rename encoded columns to meaningful names


merged_df = merged_df.rename(columns={
    "22736": "estimated_effort",
    "22737": "actual_effort",
    "40731_en": "priority",
    "40732_en": "acceptance_criteria"
})

"""##Constructing a Comprehensive Document for Each User Story with Tasks and Metadata

In this section, for each user story in the dataset, all relevant details—including the user story text, description, priority, acceptance criteria, and associated tasks—are extracted and combined into a structured document. Additionally, related metadata such as effort estimations and project identifiers are attached. These compiled documents are then stored in a DataFrame to be used later for indexing or retrieval in the RAG pipeline.

##Transforming Each Row into LangChain Document Objects

This step converts each row of the DataFrame—containing the full user story text along with tasks and metadata—into a Document object from LangChain. The page_content holds the main text, while metadata stores the user story ID. This structure is essential for preparing the data to be indexed into the vector store in the next phase.
"""

grouped_docs = []

for us_id, group in merged_df.groupby("us_id"):

    us = group.iloc[0]

    # ---- User Story section ----
    story_text = f"User Story: {us.get('us_subject_en', 'N/A')}"
    description_text = f"Description: {us.get('us_description_en', 'N/A')}"
    status_text = f"Status: {us.get('us_status_en', 'N/A')}"
    points_text = f"Story Points: {us.get('us_points_en', 'N/A')}"
    total_points_text = f"Total Story Points: {us.get('us_total_points_en', 'N/A')}"
    sprint_text = f"Sprint Order: {us.get('us_sprint_order', 'N/A')}"
    created_text = f"Created: {us.get('us_created_date_en', 'N/A')}"
    finished_text = f"Finished: {us.get('us_finished_date', 'N/A')}"
    assigned_users_text = f"Assigned Users: {us.get('us_assigned_users', 'N/A')}"

    # ---- Tasks section ----
    task_lines = []

    for _, row in group.iterrows():

        task_name = row.get("task_subject_en") or "Unknown task"

        task_status = row.get("task_status_name_en", "N/A")
        assignee = row.get("task_assigned_to", "N/A")

        created = row.get("task_created_date_en", "N/A")
        finished = row.get("task_finished_date_en", "N/A")
        due = row.get("task_due_date", "N/A")

        est = row.get("estimated_effort")
        act = row.get("actual_effort")

        est = est if pd.notna(est) else "N/A"
        act = act if pd.notna(act) else "N/A"

        task_lines.append(
            f"- Task: {task_name}\n"
            f"  Status: {task_status}\n"
            f"  Assignee: {assignee}\n"
            f"  Created: {created}\n"
            f"  Due: {due}\n"
            f"  Finished: {finished}\n"
            f"  Estimated effort: {est}\n"
            f"  Actual effort: {act}"
        )

    task_text = "Tasks:\n" + "\n".join(task_lines)

    # ---- Full document ----
    full_doc = "\n".join([
        story_text,
        description_text,
        status_text,
        points_text,
        total_points_text,
        sprint_text,
        created_text,
        finished_text,
        assigned_users_text,
        "",
        task_text
    ])

    grouped_docs.append({
        "us_id": us_id,
        "document": full_doc,
        "project_id": us.get("project_id"),
        "epic_id": us.get("epic_id"),
        "milestone_id": us.get("milestone_id"),
    })


document_df = pd.DataFrame(grouped_docs)

from langchain_core.documents import Document

documents = []

for idx, row in document_df.iterrows():

    user_story = row['document']

    # metadata
    metadata = {
        "us_id": row["us_id"],
        "source": "taiga",
        "doc_type": "user_story"
    }

    doc = Document(
        page_content=user_story,
        metadata=metadata
    )

    documents.append(doc)


for i, doc in enumerate(documents[:3]):

    print(f"\n Document {i+1}")
    print(" Content:\n", doc.page_content)
    print(" Metadata:", doc.metadata)

"""# Stage 2 — Agentic RAG Framework

## Setup
"""

# Install required packages for LangGraph Agentic RAG (Taiga Scrum Master use case)

!pip install -U  --quiet langgraph "langchain[openai]" langchain-community langchain-text-splitters

import getpass
import os

# Set OpenAI API Key (used by LangGraph + LangChain)
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OPENAI_API_KEY: ")

"""## 1. Preprocess documents

1. Fetch documents to use in our RAG system.
"""

documents[0].page_content.strip()[:1000]

"""2. Split user story documents into smaller chunks for embedding and indexing.
This improves semantic retrieval performance while preserving task-level context.
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

docs_list = documents

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=2500,     # good for user stories
    chunk_overlap=100
)

doc_splits = text_splitter.split_documents(docs_list)

doc_splits[0].page_content.strip()

"""##2. Create a retriever tool

1. Index document chunks into a vector store using embeddings
to enable semantic search and retrieval of relevant Agile data.
"""

from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

vectorstore = InMemoryVectorStore.from_documents(
    documents=doc_splits,
    embedding=OpenAIEmbeddings()
)

retriever = vectorstore.as_retriever(
    search_kwargs={"k": 8}
)

"""2. Define a tool that enables the agent to retrieve relevant Taiga user stories from the vector store using semantic search."""

from langchain.tools import tool

@tool
def retrieve_user_stories(query: str) -> str:
    """Search and return relevant Taiga user stories and their tasks."""
    docs = retriever.invoke(query)
    return "\n\n".join(doc.page_content for doc in docs)

retriever_tool = retrieve_user_stories

"""Test the tool:

"""

retriever_tool.invoke({"query": "user registration"})

"""## 3. Generate query

Decide: retrieve or respond. The LLM sees the chat history (MessagesState) and,
guided by a system prompt, either calls the Taiga retriever tool or answers directly.
"""

from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage

response_model = init_chat_model("gpt-4.1", temperature=0)

SYSTEM_PROMPT = """
You are an Agile assistant answering questions about Taiga project data.

You have access to a tool that retrieves user stories and tasks.

Use the tool when:

- the user asks about a specific user story
- the user asks about tasks, effort, assignees, sprint, story points
- the answer is not fully available in the conversation memory
- the question introduces new entities

Do NOT use the tool when:

- the question clearly refers to the immediately previous answer
- and all necessary information is already present in the conversation

Always prefer grounded answers based on retrieved context when uncertain.
"""

def generate_query_or_respond(state: MessagesState):

    messages = state["messages"]

    messages_with_system = [
        SystemMessage(content=SYSTEM_PROMPT),
        *messages
    ]

    msg = response_model.bind_tools(
        [retriever_tool]
    ).invoke(messages_with_system)

    return {"messages": [msg]}

"""Test"""

test_input = {
    "messages": [
        {"role": "user", "content": "hello!"}
    ]
}

generate_query_or_respond(test_input)["messages"][-1].pretty_print()

"""Test"""

test_input = {
    "messages": [
        {"role": "user", "content": "Show me the tasks for user registration"}
    ]
}

generate_query_or_respond(test_input)["messages"][-1].pretty_print()

"""## 4. helper function

This utility function extracts the most recent human query from the conversation history stored in the MessagesState. Since LangGraph maintains the entire multi-turn chat memory (including system, assistant, and tool messages), it is essential to identify the latest HumanMessage to ensure that retrieval and response generation are based on the current user intent rather than the initial query. This function improves correctness in multi-turn RAG interactions.
"""

from langchain_core.messages import HumanMessage

def get_last_human_text(messages) -> str:
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            return m.content
    # fallback
    return messages[0].content if messages else ""

"""##5. Grade documents

Grade retrieved documents for relevance using an LLM.
Routes to generate_answer if relevant, otherwise rewrite_question.
"""

from pydantic import BaseModel, Field
from typing import Literal
from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model


# Prompt specialized for Taiga task management data
GRADE_PROMPT = (
    "You are a grader assessing relevance of a retrieved Taiga Agile project document to a user question.\n\n"

    "The document is a Taiga user story and may contain:\n"
    "- User Story description\n"
    "- Tasks\n"
    "- Story points\n"
    "- Estimated effort\n"
    "- Actual effort\n"
    "- Assignees\n"
    "- Sprint order\n\n"

    "Retrieved document:\n"
    "{context}\n\n"

    "User question:\n"
    "{question}\n\n"

    "If the document contains information useful for answering the question "
    "(especially about tasks, effort, story points, assignment, sprint, or task management), "
    "grade it as relevant.\n\n"

    "Respond ONLY with 'yes' or 'no'."
)


class GradeDocuments(BaseModel):
    """Binary relevance score"""
    binary_score: Literal["yes", "no"] = Field(
        description="Relevance score: 'yes' if relevant, or 'no' if not relevant"
    )


grader_model = init_chat_model("gpt-4.1", temperature=0)


def grade_documents(state: MessagesState) -> Literal["generate_answer", "rewrite_question"]:
    question = get_last_human_text(state["messages"])
    context = state["messages"][-1].content

    prompt = GRADE_PROMPT.format(question=question, context=context)
    response = grader_model.with_structured_output(GradeDocuments).invoke(
        [{"role": "user", "content": prompt}]
    )
    return "generate_answer" if response.binary_score == "yes" else "rewrite_question"

"""2. Run the document grader with an IRRELEVANT Taiga-style tool response.
This tests that grade_documents detects low relevance and routes to "rewrite_question".

"""

from langchain_core.messages import convert_to_messages

# Taiga-style example input to match your dataset + your grade_documents()
example_input = {
    "messages": convert_to_messages(
        [
            {
                "role": "user",
                "content": "Which tasks in 'User registration' have actual effort higher than estimated effort?",
            },
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "1",
                        "name": "retriever_tool",  # must match your retriever tool name
                        "args": {"query": "User registration tasks actual effort estimated effort"},
                    }
                ],
            },
            {
                "role": "tool",
                "content": (
                    "User Story: User registration\n"
                    "Tasks:\n"
                    "- Task: Implementation of the C++ code for user registration\n"
                    "  Estimated effort: 4.0\n"
                    "  Actual effort: 5.0\n"
                    "- Task: Optimization of the code in C++\n"
                    "  Estimated effort: 2.0\n"
                    "  Actual effort: 1.0\n"
                ),
                "tool_call_id": "1",
            },
        ]
    )
}

grade_documents(example_input)

"""3. Test that grade_documents correctly detects a relevant Taiga document."""

from langchain_core.messages import convert_to_messages

# Taiga-aligned example input for your grade_documents()
input = {
    "messages": convert_to_messages(
        [
            {
                "role": "user",
                "content": "For the 'User registration' user story, which tasks are assigned to user 626897 and what are their estimated/actual efforts?",
            },
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "1",
                        "name": "retriever_tool",  # must match your retriever tool name
                        "args": {"query": "User registration assignee 626897 estimated effort actual effort tasks"},
                    }
                ],
            },
            {
                "role": "tool",
                "content": (
                    "User Story: User registration\n"
                    "Assigned Users: 626629.0\n\n"
                    "Tasks:\n"
                    "- Task: Implementation of the C++ code for user registration\n"
                    "  Assignee: 626897.0\n"
                    "  Estimated effort: 4.0\n"
                    "  Actual effort: 5.0\n"
                    "- Task: Optimization of the code in C++\n"
                    "  Assignee: 626897.0\n"
                    "  Estimated effort: 2.0\n"
                    "  Actual effort: 1.0\n"
                    "- Task: Design of the welcome interface (user registration)\n"
                    "  Assignee: 626897.0\n"
                    "  Estimated effort: 1.0\n"
                    "  Actual effort: 1.0\n"
                ),
                "tool_call_id": "1",
            },
        ]
    )
}

grade_documents(input)

"""## 5. Rewrite question

1. If retrieval returns irrelevant context, we rewrite the user query to improve
semantic retrieval over Taiga user stories and tasks.
The rewrite is strictly constrained to preserve the original intent and entities
(e.g., user story names, assignee IDs). If the query is already retrieval-ready,
it is returned unchanged
"""

from langchain_core.messages import HumanMessage
from langgraph.graph import MessagesState

REWRITE_PROMPT = (
    "You are rewriting a user question ONLY to improve retrieval over Taiga user stories and tasks.\n\n"
    "STRICT RULES:\n"
    "1) DO NOT change the meaning.\n"
    "2) DO NOT introduce new user story names, new entities, or new IDs.\n"
    "3) If the question mentions a specific user story (e.g., \"User login\"), you MUST keep it exactly.\n"
    "4) If the question is already clear and retrieval-ready, return it unchanged.\n"
    "5) Keep it as ONE sentence. No lists, no extra commentary.\n\n"
    "Original question:\n"
    "{question}\n\n"
    "Rewritten question (or the same question if no rewrite is needed):"
)

def rewrite_question(state: MessagesState):
    question = get_last_human_text(state["messages"]).strip()
    prompt = REWRITE_PROMPT.format(question=question)

    rewritten = response_model.invoke([{"role": "user", "content": prompt}]).content.strip()
    if not rewritten:
        rewritten = question
    return {"messages": [HumanMessage(content=rewritten)]}

"""Test"""

test_input = {
    "messages": convert_to_messages(
        [
            {
                "role": "user",
                "content": "Show me tasks for registration",
            },
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "1",
                        "name": "retrieve_user_stories",
                        "args": {"query": "registration"},
                    }
                ],
            },
            {
                "role": "tool",
                "content": "No relevant results found.",
                "tool_call_id": "1",
            },
        ]
    )
}

response = rewrite_question(test_input)

print(response["messages"][-1].content)

"""## 6. Generate an answer

Generate the final grounded answer using the retrieved Taiga context.
Hallucination is prevented via strict prompt constraints.
"""

GENERATE_PROMPT = (
    "You are an assistant for question-answering tasks over Taiga user stories.\n"
    "Use ONLY the retrieved context to answer.\n"
    "NEVER infer missing fields (e.g., priority).\n"
    "If the question asks for a field that is not explicitly present in the context, say 'Not provided in the data.'\n"
    "Do NOT treat Sprint Order as Priority unless the context explicitly says so.\n"
    "Use three sentences maximum.\n"
    "Question: {question}\n"
    "Context: {context}"
)


def generate_answer(state: MessagesState):
    question = get_last_human_text(state["messages"])
    context = state["messages"][-1].content

    prompt = GENERATE_PROMPT.format(question=question, context=context)
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [response]}

"""Test"""

test_input = {
    "messages": convert_to_messages(
        [
            {
                "role": "user",
                "content": "What tasks are included in the user registration story?",
            },
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "1",
                        "name": "retrieve_user_stories",
                        "args": {"query": "user registration"},
                    }
                ],
            },
            {
                "role": "tool",
                "content": retriever_tool.invoke({"query": "user registration"}),
                "tool_call_id": "1",
            },
        ]
    )
}

response = generate_answer(test_input)
response["messages"][-1].pretty_print()

"""## 7. Assemble the graph
Assemble the Agentic RAG workflow as a LangGraph state machine (MessagesState).
Flow:
1) START -> generate_query_or_respond: LLM decides whether retrieval is needed.
2) If tool_calls exist -> retrieve: execute retriever_tool to fetch Taiga context.
Otherwise -> END: respond directly (no retrieval).
3) After retrieval -> grade_documents: check relevance of retrieved context.
- If relevant -> generate_answer -> END (grounded final answer).
- If not relevant -> rewrite_question -> back to generate_query_or_respond (query refinement loop).

This design supports multi-turn conversations and reduces hallucinations by
combining memory-aware tool use, relevance grading, and controlled query rewriting.
"""

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import MessagesState

workflow = StateGraph(MessagesState)

# Nodes
workflow.add_node("generate_query_or_respond", generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node("rewrite_question", rewrite_question)
workflow.add_node("generate_answer", generate_answer)

# Start
workflow.add_edge(START, "generate_query_or_respond")

# If model calls tools -> retrieve, else end
workflow.add_conditional_edges(
    "generate_query_or_respond",
    tools_condition,
    {
        "tools": "retrieve",
        END: END,
    },
)

# After retrieve, grade relevance -> route
workflow.add_conditional_edges(
    "retrieve",
    grade_documents,
    {
        "generate_answer": "generate_answer",
        "rewrite_question": "rewrite_question",
    },
)

# Finish / loop
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

graph = workflow.compile()

"""Visualize the graph:

"""

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

"""## 8. Run the agentic RAG
Now let’s test the complete graph by running it with a question:

"""

for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "What tasks are included in the user registration story?",
            }
        ]
    }
):
    for node, update in chunk.items():
        print("Update from node:", node)
        update["messages"][-1].pretty_print()
        print("\n\n")

"""## 8. Add conversational memory to the graph using MemorySaver

Enable conversation memory persistence for multi-turn interactions.
"""

from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

graph = workflow.compile(checkpointer=memory)

"""## 9. Interactive chatbot interface with memory-enabled Agentic RAG graph

Launch an interactive chat session with memory and real-time graph execution.
"""

import builtins
from langchain_core.messages import HumanMessage

def start_chatbot(thread_id: str = "taiga-thread-1", show_tool_output: bool = True, tool_preview_chars: int = 1200):
    print("Hi! I'm your Agile Task-Management assistant for Taiga user stories.")
    print("Ask me about tasks, effort (estimated vs actual), assignees, story points, sprint order, or status.")
    print("Type 'exit' to quit.\n")

    config = {"configurable": {"thread_id": thread_id}}

    while True:
        user_input = builtins.input("You: ").strip()
        if user_input.lower() in {"exit", "quit"}:
            print("Goodbye! Good luck with your project.")
            break

        # Stream through the graph (keeps chat history via MemorySaver + thread_id)
        for step in graph.stream(
            {"messages": [HumanMessage(content=user_input)]},
            stream_mode="values",
            config=config,
        ):
            last_msg = step["messages"][-1]
            msg_type = getattr(last_msg, "type", None)

            # 1) Show tool output (retrieved context)
            if msg_type == "tool":
                if show_tool_output:
                    print("\n Tool Output (retrieved context):")
                    content = last_msg.content or ""
                    if len(content) > tool_preview_chars:
                        print(content[:tool_preview_chars] + "\n... [truncated]")
                    else:
                        print(content)
                continue

            # 2) Show assistant messages (including tool calls)
            if msg_type == "ai":
                # If the AI message contains tool calls, label it clearly
                if getattr(last_msg, "tool_calls", None):
                    print("\n Assistant decided to call a tool:")
                else:
                    print("\n Assistant:")

                last_msg.pretty_print()
                continue

            # 3) Show any other message types (human/system) if they appear
            print("\n Message:")
            last_msg.pretty_print()

        print("\n Next question? (or type 'exit' to quit)\n")

start_chatbot(show_tool_output=True, tool_preview_chars=600)

